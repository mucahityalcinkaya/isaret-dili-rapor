# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NSni-sSlDWo6DqZmFPydzLk0PmTUIdv7
"""

# ============================================================================
# HÃœCRE 1: Google Drive BaÄŸlantÄ±sÄ± ve Gerekli KÃ¼tÃ¼phaneler
# ============================================================================
from google.colab import drive
drive.mount('/content/drive')

# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle
!pip uninstall -y mediapipe
!pip install -q mediapipe opencv-python tqdm

print("âœ“ KÃ¼tÃ¼phaneler yÃ¼klendi - ÅŸimdi runtime'Ä± restart edin!")
print("âš ï¸ Runtime â†’ Restart Runtime yapÄ±n ve sonraki hÃ¼creye geÃ§in")

# ============================================================================
# HÃœCRE 1.5: Import (Runtime restart sonrasÄ± Ã§alÄ±ÅŸtÄ±rÄ±n)
# ============================================================================
import os
import json
import random
import cv2
import numpy as np
from tqdm import tqdm
import zipfile
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import torch

# MediaPipe sadece ihtiyacÄ±mÄ±z olan modÃ¼lleri import et
from mediapipe.python.solutions import holistic as mp_holistic

# GPU kontrolÃ¼
print("âœ“ TÃ¼m kÃ¼tÃ¼phaneler hazÄ±r")
print(f"ğŸ® GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'YOK'}")
print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB" if torch.cuda.is_available() else "")

from google.colab import drive
drive.mount('/content/drive')

# ============================================================================
# HÃœCRE 2: Zip DosyasÄ±nÄ± Ã‡Ä±kar
# ============================================================================
# Drive'daki zip dosyanÄ±zÄ±n yolunu buraya girin
ZIP_PATH = "/content/drive/MyDrive/train_veriduzeltilmis.zip"  # KENDI YOLUNUZU GÄ°RÄ°N

# Ã‡Ä±karma dizini
EXTRACT_DIR = "/content/videos"
OUTPUT_DIR = "/content/unisiggn"

# Zip'i Ã§Ä±kar
print("ğŸ“¦ Zip dosyasÄ± Ã§Ä±karÄ±lÄ±yor...")
os.makedirs(EXTRACT_DIR, exist_ok=True)
with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
    zip_ref.extractall(EXTRACT_DIR)

print(f"âœ“ Dosyalar Ã§Ä±karÄ±ldÄ±: {EXTRACT_DIR}")
print(f"Ä°Ã§erik: {os.listdir(EXTRACT_DIR)}")

# ============================================================================
# HÃœCRE 3: Ayarlar ve Fonksiyonlar
# ============================================================================
# AYARLAR
TARGET_T = 64
VAL_RATIO = 0.20
SEED = 42
OUTLIER_ABS_THR = 10.0
OUTLIER_MODE = "skip"

# A100 iÃ§in Ã§oklu thread ayarlarÄ± (MediaPipe CPU'da ama paralel okuma yapabiliriz)
NUM_THREADS = 8  # Paralel video okuma thread'i
PROCESS_BATCH = 16  # Bir anda iÅŸlenecek video sayÄ±sÄ±

# MediaPipe keypoint seÃ§imleri
IDX_LH = list(range(21))
IDX_RH = list(range(21))
IDX_BODY = [0, 7, 8, 11, 12, 13, 14, 15, 16]
ROOT_LH = 0
ROOT_RH = 0

# YardÄ±mcÄ± fonksiyonlar
def list_classes(root):
    return sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])

def uniform_indices(n_frames, T):
    if n_frames <= 0:
        return None
    if n_frames == 1:
        return np.zeros((T,), dtype=np.int64)
    return np.linspace(0, n_frames - 1, T).round().astype(np.int64)

def safe_makedirs(p):
    os.makedirs(p, exist_ok=True)

def pick_train_val(samples, val_ratio, seed):
    rnd = random.Random(seed)
    out_train, out_val = [], []
    by_cls = {}
    for s in samples:
        by_cls.setdefault(s["cls"], []).append(s)
    for cls, arr in by_cls.items():
        rnd.shuffle(arr)
        n = len(arr)
        n_val = max(1, int(round(n * val_ratio))) if n >= 5 else (1 if n >= 2 else 0)
        out_val.extend(arr[:n_val])
        out_train.extend(arr[n_val:])
    return out_train, out_val

print(f"âœ“ Ayarlar hazÄ±r - {NUM_THREADS} paralel thread kullanÄ±lacak")

# ============================================================================
# HÃœCRE 4: MediaPipe Model Pool (Thread-Safe)
# ============================================================================
class HolisticPool:
    """Thread-safe MediaPipe Holistic model pool"""
    def __init__(self, pool_size=8):
        self.pool_size = pool_size
        self.models = []
        self.lock = Lock()

        # Model pool'u oluÅŸtur
        for _ in range(pool_size):
            model = mp_holistic.Holistic(
                static_image_mode=False,
                model_complexity=0,  # Lite = daha hÄ±zlÄ±
                enable_segmentation=False,
                refine_face_landmarks=False,
                min_detection_confidence=0.3,
                min_tracking_confidence=0.3
            )
            self.models.append(model)
        print(f"âœ“ {pool_size} MediaPipe model havuzu oluÅŸturuldu")

    def get_model(self):
        """Thread-safe model al"""
        with self.lock:
            if self.models:
                return self.models.pop()
        # Pool boÅŸsa yeni model oluÅŸtur
        return mp_holistic.Holistic(
            static_image_mode=False,
            model_complexity=0,
            enable_segmentation=False,
            refine_face_landmarks=False,
            min_detection_confidence=0.3,
            min_tracking_confidence=0.3
        )

    def return_model(self, model):
        """Thread-safe model geri ver"""
        with self.lock:
            if len(self.models) < self.pool_size:
                self.models.append(model)
            else:
                model.close()

    def close_all(self):
        """TÃ¼m modelleri kapat"""
        with self.lock:
            for model in self.models:
                model.close()
            self.models.clear()

def extract_mediapipe_keypoints(holistic, bgr_img):
    """MediaPipe'tan pose ve el keypoints Ã§Ä±kar"""
    H, W = bgr_img.shape[:2]

    # GPU'da decode edilmiÅŸ frame'i direkt kullan
    rgb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
    results = holistic.process(rgb)

    output = {
        'pose': np.zeros((33, 3), dtype=np.float32),
        'left_hand': np.zeros((21, 3), dtype=np.float32),
        'right_hand': np.zeros((21, 3), dtype=np.float32)
    }

    if results.pose_landmarks:
        for i, lm in enumerate(results.pose_landmarks.landmark):
            output['pose'][i] = [lm.x * W, lm.y * H, lm.visibility]

    if results.left_hand_landmarks:
        for i, lm in enumerate(results.left_hand_landmarks.landmark):
            output['left_hand'][i] = [lm.x * W, lm.y * H, 1.0]

    if results.right_hand_landmarks:
        for i, lm in enumerate(results.right_hand_landmarks.landmark):
            output['right_hand'][i] = [lm.x * W, lm.y * H, 1.0]

    return output

def select_and_normalize(kpts_dict, W, H):
    """Normalize ve root subtraction"""
    pose = kpts_dict['pose'].copy()
    lh = kpts_dict['left_hand'].copy()
    rh = kpts_dict['right_hand'].copy()

    # Normalize
    pose[:, 0] /= max(W, 1)
    pose[:, 1] /= max(H, 1)
    lh[:, 0] /= max(W, 1)
    lh[:, 1] /= max(H, 1)
    rh[:, 0] /= max(W, 1)
    rh[:, 1] /= max(H, 1)

    bd = pose[IDX_BODY]

    # Root subtract
    lh[:, :2] -= lh[ROOT_LH, :2]
    rh[:, :2] -= rh[ROOT_RH, :2]

    # Shoulder scale
    ls, rs = pose[11, :2], pose[12, :2]
    sd = np.linalg.norm(ls - rs)
    sd = float(np.clip(sd, 0.05, 0.8)) if np.isfinite(sd) and sd > 1e-4 else 1.0

    lh[:, :2] /= sd
    rh[:, :2] /= sd
    bd[:, :2] /= sd

    out = np.concatenate([lh, rh, bd], axis=0).astype(np.float32)
    out = np.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)

    if np.max(np.abs(out[:, :2])) > OUTLIER_ABS_THR:
        if OUTLIER_MODE == "clip":
            out[:, :2] = np.clip(out[:, :2], -OUTLIER_ABS_THR, OUTLIER_ABS_THR)
        else:
            return None
    return out

def process_video_fast(video_path, out_npy_path, model_pool):
    """HÄ±zlandÄ±rÄ±lmÄ±ÅŸ video iÅŸleme"""
    # Model pool'dan model al
    holistic = model_pool.get_model()

    try:
        # OpenCV GPU backend kullan (CUDA varsa)
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            model_pool.return_model(holistic)
            return False

        n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        idxs = uniform_indices(n_frames, TARGET_T)
        if idxs is None:
            cap.release()
            model_pool.return_model(holistic)
            return False

        frames_out = []
        last_ok = None

        # Frame'leri oku ve iÅŸle
        for ti in idxs:
            cap.set(cv2.CAP_PROP_POS_FRAMES, int(ti))
            ok, frame = cap.read()
            if not ok:
                if last_ok is None:
                    frames_out.append(np.zeros((51, 3), dtype=np.float32))
                else:
                    frames_out.append(last_ok.copy())
                continue

            kpts_dict = extract_mediapipe_keypoints(holistic, frame)
            k51 = select_and_normalize(kpts_dict, W, H)
            if k51 is None:
                cap.release()
                model_pool.return_model(holistic)
                return False

            frames_out.append(k51)
            last_ok = k51

        cap.release()

        # Sonucu kaydet
        arr = np.stack(frames_out, axis=0).astype(np.float32)
        np.save(out_npy_path, arr)

        # Modeli geri ver
        model_pool.return_model(holistic)
        return True

    except Exception as e:
        model_pool.return_model(holistic)
        return False

print("âœ“ MediaPipe fonksiyonlarÄ± hazÄ±r (Thread-safe)")

# ============================================================================
# HÃœCRE 5: Video KlasÃ¶rÃ¼nÃ¼ Bul ve Sample List OluÅŸtur
# ============================================================================
# Ã‡Ä±karÄ±lan klasÃ¶rde video klasÃ¶rÃ¼nÃ¼ bul
contents = os.listdir(EXTRACT_DIR)
if len(contents) == 1 and os.path.isdir(os.path.join(EXTRACT_DIR, contents[0])):
    VIDEO_ROOT = os.path.join(EXTRACT_DIR, contents[0])
else:
    VIDEO_ROOT = EXTRACT_DIR

print(f"ğŸ“‚ Video dizini: {VIDEO_ROOT}")

# Output dizinini hazÄ±rla
safe_makedirs(OUTPUT_DIR)
skel_root = os.path.join(OUTPUT_DIR, "skeleton")
safe_makedirs(skel_root)

# SÄ±nÄ±flarÄ± listele
classes = list_classes(VIDEO_ROOT)
print(f"ğŸ“Š SÄ±nÄ±f sayÄ±sÄ±: {len(classes)}")
print(f"SÄ±nÄ±flar: {classes[:5]}..." if len(classes) > 5 else f"SÄ±nÄ±flar: {classes}")

# class_to_idx kaydet
class_to_idx = {c: i for i, c in enumerate(classes)}
with open(os.path.join(OUTPUT_DIR, "class_to_idx.json"), "w", encoding="utf-8") as f:
    json.dump(class_to_idx, f, ensure_ascii=False, indent=2)

# Sample listesi oluÅŸtur
samples = []
total_videos = 0
for cls in classes:
    cdir = os.path.join(VIDEO_ROOT, cls)
    videos = [f for f in os.listdir(cdir) if f.lower().endswith((".mp4", ".avi", ".mov", ".mkv"))]
    total_videos += len(videos)
    for fn in videos:
        stem = os.path.splitext(fn)[0]
        samples.append({"cls": cls, "video": os.path.join(cdir, fn), "stem": stem})

print(f"ğŸ¥ Toplam video sayÄ±sÄ±: {total_videos}")

# Train/Val split
train_s, val_s = pick_train_val(samples, VAL_RATIO, SEED)
print(f"ğŸ“š Train: {len(train_s)}, Val: {len(val_s)}")

# Split dosyalarÄ±nÄ± kaydet
safe_makedirs(os.path.join(OUTPUT_DIR, "splits"))
with open(os.path.join(OUTPUT_DIR, "splits", "train.txt"), "w", encoding="utf-8") as f:
    for s in train_s:
        f.write(f"{s['cls']}/{s['stem']}\n")
with open(os.path.join(OUTPUT_DIR, "splits", "val.txt"), "w", encoding="utf-8") as f:
    for s in val_s:
        f.write(f"{s['cls']}/{s['stem']}\n")

print("âœ“ Sample listesi ve split dosyalarÄ± hazÄ±r")

# ============================================================================
# HÃœCRE 6: Keypoint Ã‡Ä±karma (PARALEL THREAD - A100 Optimized)
# ============================================================================
print("ğŸš€ PARALEL keypoint Ã§Ä±karma baÅŸlÄ±yor...")
print(f"âš¡ A100: {NUM_THREADS} paralel thread kullanÄ±lÄ±yor")
print(f"ğŸ’¡ Model pool boyutu: {NUM_THREADS}")

# OpenCV CUDA backend'i etkinleÅŸtir
cv2.setNumThreads(4)  # Her thread iÃ§in 4 CPU thread

random.seed(SEED)

# Model pool oluÅŸtur
model_pool = HolisticPool(pool_size=NUM_THREADS)

# Ä°ÅŸlenecek videolarÄ± hazÄ±rla
tasks = []
for s in samples:
    out_dir = os.path.join(skel_root, s["cls"])
    safe_makedirs(out_dir)
    out_path = os.path.join(out_dir, s["stem"] + ".npy")

    # Zaten iÅŸlenmiÅŸse atla
    if not os.path.exists(out_path):
        tasks.append((s["video"], out_path))

print(f"ğŸ“‹ Ä°ÅŸlenecek video sayÄ±sÄ±: {len(tasks)}")

if len(tasks) == 0:
    print("âœ“ TÃ¼m videolar zaten iÅŸlenmiÅŸ!")
else:
    # Paralel iÅŸleme
    ok_cnt = 0
    bad_cnt = 0

    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        # TÃ¼m tasklarÄ± gÃ¶nder
        futures = {
            executor.submit(process_video_fast, video_path, out_path, model_pool): (video_path, out_path)
            for video_path, out_path in tasks
        }

        # Progress bar ile takip et
        with tqdm(total=len(tasks), desc="ğŸ¬ Processing", ncols=100) as pbar:
            for future in as_completed(futures):
                result = future.result()
                if result:
                    ok_cnt += 1
                else:
                    bad_cnt += 1
                pbar.update(1)
                pbar.set_postfix({'âœ“': ok_cnt, 'âœ—': bad_cnt})

    # Model pool'u kapat
    model_pool.close_all()

    print("\n" + "="*60)
    print("âœ… Ä°ÅLEM TAMAMLANDI")
    print("="*60)
    print(f"âœ“ BaÅŸarÄ±lÄ±:     {ok_cnt}")
    print(f"âœ— BaÅŸarÄ±sÄ±z:    {bad_cnt}")
    print(f"ğŸ“ Ã‡Ä±ktÄ± dizini: {OUTPUT_DIR}")
    print("="*60)

# ============================================================================
# HÃœCRE 7: SonuÃ§larÄ± Drive'a Kaydet
# ============================================================================
print("ğŸ’¾ SonuÃ§lar Drive'a kaydediliyor...")

# Output'u ziple
output_zip = "/content/unisiggn_output.zip"
shutil.make_archive(output_zip.replace('.zip', ''), 'zip', OUTPUT_DIR)

# Drive'a kopyala
drive_output = "/content/drive/MyDrive/unisiggn_output.zip"
shutil.copy(output_zip, drive_output)

print(f"âœ“ SonuÃ§lar Drive'a kaydedildi: {drive_output}")
print(f"ğŸ“¦ Zip boyutu: {os.path.getsize(drive_output) / (1024*1024):.2f} MB")

import glob

npy_files = glob.glob(os.path.join(skel_root, "*/*.npy"))
if npy_files:
    sample_file = random.choice(npy_files)
    data = np.load(sample_file)
    print(f"\nğŸ” Ã–rnek dosya kontrolÃ¼:")
    print(f"Dosya: {os.path.basename(sample_file)}")
    print(f"Shape: {data.shape}")  # Beklenen: (64, 51, 3)
    print(f"Min: {data.min():.4f}, Max: {data.max():.4f}")
    print(f"Mean: {data.mean():.4f}, Std: {data.std():.4f}")
    print(f"\nÄ°lk frame'in ilk 5 keypoint:")
    print(data[0, :5, :])

    # GPU memory kullanÄ±mÄ±
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ GPU Memory kullanÄ±mÄ±: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
else:
    print("âš ï¸ Ä°ÅŸlenmiÅŸ dosya bulunamadÄ±!")

# Ä°lk 10 frame'i detaylÄ± incele
sample_file = "/content/unisiggn/skeleton/abdomen/00333.npy"
data = np.load(sample_file)

print(f"ğŸ“ Dosya: {os.path.basename(sample_file)}")
print(f"ğŸ“Š Shape: {data.shape}\n")

# Ä°lk 10 frame'i kontrol et
for frame_idx in range(10):
    frame_data = data[frame_idx]

    # Her keypoint grubunu kontrol et
    lh = frame_data[:21]   # Left hand
    rh = frame_data[21:42] # Right hand
    bd = frame_data[42:]   # Body

    lh_valid = (lh != 0).any(axis=1).sum()
    rh_valid = (rh != 0).any(axis=1).sum()
    bd_valid = (bd != 0).any(axis=1).sum()

    print(f"Frame {frame_idx:2d}: LH={lh_valid:2d}/21, RH={rh_valid:2d}/21, Body={bd_valid}/9 | "
          f"Min={frame_data.min():6.3f}, Max={frame_data.max():6.3f}, Mean={frame_data.mean():6.3f}")

print("\n" + "="*70)
print("ğŸ“Š Frame 0 detaylÄ± (ilk 5 keypoint her gruptan):")
print("="*70)
print("\nğŸ–ï¸ Sol El (Left Hand) - Ä°lk 5:")
print(data[0, :5])
print("\nğŸ–ï¸ SaÄŸ El (Right Hand) - Ä°lk 5:")
print(data[0, 21:26])
print("\nğŸ§ Body - TÃ¼mÃ¼ (9 keypoint):")
print(data[0, 42:])