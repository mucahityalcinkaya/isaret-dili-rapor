# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BLtwWsmvqJWmxrT8a_V6CKC4s9_wU0Ud
"""

# ============================================================================
# EMERGENCY: Clear Memory
# ============================================================================

import torch
import gc

print("üßπ Clearing GPU memory...")

# Model'i sil
if 'model' in locals():
    del model
if 'trainer' in locals():
    del trainer

# Cache temizle
torch.cuda.empty_cache()
gc.collect()

# Memory durumu
print(f"‚úÖ Memory cleared!")
print(f"   Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"   Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
print(f"   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.2f} GB")

# SONRA: Runtime > Restart Runtime
print("\n‚ö†Ô∏è √ñNEMLI: Runtime > Restart Runtime yapƒ±n!")

# ============================================================================
# CELL 1: CLEAN SETUP (NO BITSANDBYTES)
# ============================================================================

print("üöÄ Clean setup for A100...")

!pip install -q transformers==4.46.0
!pip install -q datasets==3.1.0
!pip install -q peft==0.13.2
!pip install -q trl==0.12.1
!pip install -q accelerate==1.1.1
!pip install -q wandb

print("‚úÖ Ready! (BitsAndBytes not needed for A100 full precision)")

# ============================================================================
# CELL 2: Import Libraries
# ============================================================================

import os
import json
import torch
import wandb
from datetime import datetime
from typing import Dict, List

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
)
from peft import (
    LoraConfig,
    get_peft_model,
    PeftModel
)
from datasets import load_dataset, Dataset
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

print("üìö Libraries imported successfully!")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print(f"Compute Capability: {torch.cuda.get_device_capability(0)}")

# ============================================================================
# CELL 3: Configuration - MEMORY OPTIMIZED
# ============================================================================

import os, shutil
from datetime import datetime
from google.colab import drive

# Drive baƒüla ve qwen_ready √ßek
DRIVE_QWEN_READY_DIR = "/content/drive/MyDrive/qwen_ready"
LOCAL_QWEN_READY_DIR = "/content/qwen_ready"

if not os.path.exists(DRIVE_QWEN_READY_DIR):
    raise FileNotFoundError(f"Drive'da bulunamadƒ±: {DRIVE_QWEN_READY_DIR}")

if os.path.exists(LOCAL_QWEN_READY_DIR):
    shutil.rmtree(LOCAL_QWEN_READY_DIR)

shutil.copytree(DRIVE_QWEN_READY_DIR, LOCAL_QWEN_READY_DIR)
print(f"‚úÖ qwen_ready Drive'dan √ßekildi: {LOCAL_QWEN_READY_DIR}")

class Config:
    # Paths
    DATA_DIR = "/content/qwen_ready"
    TRAIN_FILE = f"{DATA_DIR}/qwen_train.jsonl"
    VAL_FILE   = f"{DATA_DIR}/qwen_val.jsonl"
    TEST_FILE  = f"{DATA_DIR}/qwen_test.jsonl"

    # Model
    MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"
    OUTPUT_DIR = "./qwen-medical-sign-lora"

    # LoRA Config - REDUCED for memory
    LORA_R = 32  # 64 ‚Üí 32 (yarƒ± rank)
    LORA_ALPHA = 64  # 128 ‚Üí 64
    LORA_DROPOUT = 0.05
    LORA_TARGET_MODULES = [
        "q_proj", "v_proj",  # Sadece 2 mod√ºl (7 yerine)
    ]

    # Training - MEMORY OPTIMIZED
    BATCH_SIZE = 4  # 16 ‚Üí 4 (√∂nemli!)
    GRADIENT_ACCUMULATION_STEPS = 8  # 2 ‚Üí 8 (aynƒ± effective batch)
    LEARNING_RATE = 3e-4
    NUM_EPOCHS = 4
    WARMUP_RATIO = 0.05
    WEIGHT_DECAY = 0.01
    MAX_SEQ_LENGTH = 1024  # 2048 ‚Üí 1024 (yarƒ± sequence)

    # Evaluation & Logging
    EVAL_STEPS = 50
    SAVE_STEPS = 100
    LOGGING_STEPS = 10
    SAVE_TOTAL_LIMIT = 2  # 5 ‚Üí 2 (daha az checkpoint)

    # Memory Optimization
    USE_GRADIENT_CHECKPOINTING = True  # False ‚Üí True
    OPTIM = "adamw_8bit"  # fused ‚Üí 8bit (daha az memory)
    FP16 = False
    BF16 = True
    TF32 = True

    # Tracking
    WANDB_PROJECT = "qwen-medical-sign"
    WANDB_RUN_NAME = f"qwen-3b-memory-opt-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

config = Config()

# TF32 enable
import torch
if config.TF32:
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

print("‚öôÔ∏è Memory-Optimized Configuration!")
print(f"Batch size: {config.BATCH_SIZE} (was 16)")
print(f"Grad accum: {config.GRADIENT_ACCUMULATION_STEPS} (was 2)")
print(f"Effective batch: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS} (same: 32)")
print(f"LoRA rank: {config.LORA_R} (was 64)")
print(f"Max seq: {config.MAX_SEQ_LENGTH} (was 2048)")
print(f"Gradient checkpointing: {config.USE_GRADIENT_CHECKPOINTING}")

# ============================================================================
# CELL 4: Load Dataset
# ============================================================================

def load_jsonl_dataset(file_path: str) -> Dataset:
    """Load JSONL file as HuggingFace Dataset"""
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return Dataset.from_list(data)

print("üìÇ Loading datasets...")

train_dataset = load_jsonl_dataset(config.TRAIN_FILE)
val_dataset = load_jsonl_dataset(config.VAL_FILE)
test_dataset = load_jsonl_dataset(config.TEST_FILE)

print(f"\n‚úÖ Datasets loaded!")
print(f"  Training:   {len(train_dataset)} examples")
print(f"  Validation: {len(val_dataset)} examples")
print(f"  Test:       {len(test_dataset)} examples")
print(f"\nüìä Training stats:")
print(f"  Steps per epoch: ~{len(train_dataset) // (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS)}")
print(f"  Total steps: ~{(len(train_dataset) // (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS)) * config.NUM_EPOCHS}")

# ============================================================================
# CELL 5: Load Tokenizer
# ============================================================================

print("üî§ Loading tokenizer...")

tokenizer = AutoTokenizer.from_pretrained(
    config.MODEL_NAME,
    trust_remote_code=True,
    padding_side="right"
)

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print(f"‚úÖ Tokenizer loaded!")
print(f"  Vocab size: {len(tokenizer)}")

# CELL 5.5: Dataset Debug
print("üîç DEBUG: Checking dataset structure...")
print("\n1. Column names:")
print(f"   {train_dataset.column_names}")

print("\n2. First example:")
print(train_dataset[0])

print("\n3. Features:")
print(train_dataset.features)

print("\n4. Example keys:")
if len(train_dataset) > 0:
    print(f"   {list(train_dataset[0].keys())}")

# ============================================================================
# CELL 6: Format Dataset - FIXED
# ============================================================================

print("üìÑ Formatting datasets from messages to text...")

def format_chat_template(example):
    """Convert messages to Qwen chat format"""
    conversation = tokenizer.apply_chat_template(
        example["messages"],
        tokenize=False,
        add_generation_prompt=False
    )
    return {"text": str(conversation)}

print(f"Train columns before: {train_dataset.column_names}")

# Format i≈ülemi
train_dataset = train_dataset.map(
    format_chat_template,
    remove_columns=train_dataset.column_names,
    num_proc=1,  # Tek process (hata kontrol√º i√ßin)
    desc="Formatting train"
)

val_dataset = val_dataset.map(
    format_chat_template,
    remove_columns=val_dataset.column_names,
    num_proc=1,
    desc="Formatting validation"
)

test_dataset = test_dataset.map(
    format_chat_template,
    remove_columns=test_dataset.column_names,
    num_proc=1,
    desc="Formatting test"
)

print(f"‚úÖ Datasets formatted!")
print(f"Train columns after: {train_dataset.column_names}")
print(f"  Training:   {len(train_dataset)} examples")
print(f"  Validation: {len(val_dataset)} examples")
print(f"  Test:       {len(test_dataset)} examples")

# Validasyon
print("\nüîç Checking first example:")
sample = train_dataset[0]
print(f"  Type: {type(sample['text'])}")
print(f"  Length: {len(sample['text'])} chars")
print(f"  Preview:\n{sample['text'][:400]}...")

# Tokenization test
print("\nüß™ Testing tokenization...")
tokens = tokenizer(sample['text'], truncation=True, max_length=config.MAX_SEQ_LENGTH)
print(f"  Tokens: {len(tokens['input_ids'])}")
print(f"  Max length: {config.MAX_SEQ_LENGTH}")
print("  ‚úÖ Tokenization works!")

# ============================================================================
# CELL 7: Load Model - MEMORY SAFE
# ============================================================================

print("ü§ñ Loading model with memory optimization...")

import torch

# Memory temizle √∂nce
torch.cuda.empty_cache()
import gc
gc.collect()

# Model y√ºkle - gradient checkpointing ile
model = AutoModelForCausalLM.from_pretrained(
    config.MODEL_NAME,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    attn_implementation="eager",
    use_cache=False  # Gradient checkpointing i√ßin
)

# Gradient checkpointing aktif et
if config.USE_GRADIENT_CHECKPOINTING:
    model.gradient_checkpointing_enable()
    print("‚úÖ Gradient checkpointing enabled")

print(f"‚úÖ Model loaded!")
print(f"  Parameters: {model.num_parameters() / 1e9:.2f}B")
print(f"  Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

# ============================================================================
# CELL 8: Setup LoRA - FIXED
# ============================================================================

print("üéØ Setting up LoRA with high rank...")

# √ñnce modelin hangi mod√ºlleri olduƒüunu kontrol et
print("\nüîç Checking model architecture...")
for name, module in model.named_modules():
    if 'proj' in name.lower() or 'mlp' in name.lower():
        print(f"  Found: {name}")
        if len([n for n, _ in model.named_modules() if 'proj' in n.lower()]) > 20:
            break  # ƒ∞lk birka√ßƒ±nƒ± g√∂ster

# Qwen 2.5 i√ßin DOƒûRU target modules
lora_config = LoraConfig(
    r=config.LORA_R,
    lora_alpha=config.LORA_ALPHA,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    lora_dropout=config.LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM",
    modules_to_save=None  # Ekstra stabilite i√ßin
)

# Model √ºzerine LoRA uygula
try:
    model = get_peft_model(model, lora_config)
    print("‚úÖ LoRA successfully applied!")
except Exception as e:
    print(f"‚ùå Error: {e}")
    print("\nüîß Trying alternative target modules...")

    # Alternatif: Daha az mod√ºl
    lora_config = LoraConfig(
        r=config.LORA_R,
        lora_alpha=config.LORA_ALPHA,
        target_modules=["q_proj", "v_proj"],  # Minimal ama √ßalƒ±≈üƒ±r
        lora_dropout=config.LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora_config)
    print("‚úÖ LoRA applied with minimal modules!")

# ƒ∞statistikler
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())

print(f"\nüìä LoRA Statistics:")
print(f"  LoRA rank: {config.LORA_R}")
print(f"  LoRA alpha: {config.LORA_ALPHA}")
print(f"  Trainable: {trainable_params:,} ({100*trainable_params/total_params:.3f}%)")
print(f"  Total: {total_params:,}")
print(f"  Memory saved: ~{100*(1-trainable_params/total_params):.1f}%")

# ============================================================================
# CELL 9: Training Arguments - A100 Optimized
# ============================================================================

print("‚öôÔ∏è  Setting up A100-optimized training arguments...")

training_args = TrainingArguments(
    output_dir=config.OUTPUT_DIR,

    # Training
    num_train_epochs=config.NUM_EPOCHS,
    per_device_train_batch_size=config.BATCH_SIZE,
    per_device_eval_batch_size=config.BATCH_SIZE,
    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,

    # Optimization - A100 specific
    learning_rate=config.LEARNING_RATE,
    weight_decay=config.WEIGHT_DECAY,
    warmup_ratio=config.WARMUP_RATIO,
    optim=config.OPTIM,  # Fused optimizer for A100
    max_grad_norm=1.0,

    # Precision - A100 best settings
    fp16=config.FP16,
    bf16=config.BF16,
    tf32=config.TF32,

    # Evaluation
    eval_strategy="steps",
    eval_steps=config.EVAL_STEPS,
    save_strategy="steps",
    save_steps=config.SAVE_STEPS,
    save_total_limit=config.SAVE_TOTAL_LIMIT,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    # Logging
    logging_steps=config.LOGGING_STEPS,
    logging_dir=f"{config.OUTPUT_DIR}/logs",
    report_to="wandb",

    # Speed optimization
    dataloader_num_workers=4,  # Parallel data loading
    dataloader_pin_memory=True,
    gradient_checkpointing=config.USE_GRADIENT_CHECKPOINTING,

    # Other
    remove_unused_columns=False,
    seed=42,
    ddp_find_unused_parameters=False,
)

print("‚úÖ Training arguments configured for A100!")
print(f"  Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}")
print(f"  Optimizer: {config.OPTIM} (fused for speed)")
print(f"  Precision: BF16 + TF32")

# ============================================================================
# CELL 10: Initialize Trainer (FINAL FIX)
# ============================================================================

print("üë®‚Äçüè´ Initializing trainer...")

# Pad token d√ºzeltmesi
if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:
    tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})
    model.resize_token_embeddings(len(tokenizer))
    print(f"‚úÖ Added pad token: {tokenizer.pad_token}")

# DOƒûRU COLLATOR - features direkt string olabilir
from transformers import DataCollatorForLanguageModeling

class SimpleTextCollator:
    def __init__(self, tokenizer, max_length):
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __call__(self, features):
        # Features dict veya string olabilir
        if isinstance(features[0], dict):
            texts = [f["text"] for f in features]
        elif isinstance(features[0], str):
            texts = features
        else:
            # Dataset'ten gelen format
            texts = [f.get("text", f) for f in features]

        # Tokenize
        batch = self.tokenizer(
            texts,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )

        # Labels
        batch["labels"] = batch["input_ids"].clone()
        batch["labels"][batch["labels"] == self.tokenizer.pad_token_id] = -100

        return batch

collator = SimpleTextCollator(tokenizer, config.MAX_SEQ_LENGTH)

# SFTConfig
from trl import SFTConfig

sft_config = SFTConfig(
    output_dir=config.OUTPUT_DIR,
    num_train_epochs=config.NUM_EPOCHS,
    per_device_train_batch_size=config.BATCH_SIZE,
    per_device_eval_batch_size=config.BATCH_SIZE,
    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,
    learning_rate=config.LEARNING_RATE,
    weight_decay=config.WEIGHT_DECAY,
    warmup_ratio=config.WARMUP_RATIO,
    bf16=True,
    logging_steps=config.LOGGING_STEPS,
    eval_strategy="steps",
    eval_steps=config.EVAL_STEPS,
    save_strategy="steps",
    save_steps=config.SAVE_STEPS,
    save_total_limit=config.SAVE_TOTAL_LIMIT,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    report_to="none",
    dataloader_num_workers=0,  # 0 yap (multiprocessing sorunu y√ºz√ºnden)
    remove_unused_columns=False,  # FALSE - bu √∂nemli!
    max_seq_length=config.MAX_SEQ_LENGTH,
    dataset_text_field="text",
    packing=False,
)

trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=collator,
)

print("‚úÖ Trainer initialized!")

# Test
try:
    test_batch = next(iter(trainer.get_train_dataloader()))
    print(f"‚úÖ Test batch shape: {test_batch['input_ids'].shape}")
    print(f"‚úÖ Labels shape: {test_batch['labels'].shape}")
    print(f"‚úÖ Ready to train!")
except Exception as e:
    print(f"‚ùå Test failed: {e}")
    import traceback
    traceback.print_exc()

# ============================================================================
# CELL 11: Start Training
# ============================================================================

print("üéì Starting training on A100...")
print("=" * 80)

# Start time
import time
start_time = time.time()

# Train!
try:
    train_result = trainer.train()

    # End time
    elapsed_time = time.time() - start_time

    # Save
    trainer.save_model()
    trainer.save_state()

    print("\n" + "=" * 80)
    print("‚úÖ TRAINING COMPLETE!")
    print("=" * 80)
    print(f"üéØ Final train loss: {train_result.training_loss:.4f}")
    print(f"‚è±Ô∏è Training time: {elapsed_time/60:.1f} minutes")
    print(f"üöÄ Samples/sec: {train_result.metrics.get('train_samples_per_second', 0):.2f}")
    print(f"üíæ Model saved to: {config.OUTPUT_DIR}")

except Exception as e:
    print(f"\n‚ùå Training error: {e}")
    import traceback
    traceback.print_exc()

    # Save checkpoint if possible
    try:
        trainer.save_model(f"{config.OUTPUT_DIR}/checkpoint-error")
        print(f"üíæ Emergency checkpoint saved")
    except:
        pass



# ============================================================================
# FIX: Load tokenizer from checkpoint + resize base model, then load LoRA
# ============================================================================

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from trl import SFTTrainer

CKPT_PATH = f"{config.OUTPUT_DIR}/checkpoint-300"

print("üîÑ Loading tokenizer from checkpoint:", CKPT_PATH)
tokenizer_ckpt = AutoTokenizer.from_pretrained(CKPT_PATH, trust_remote_code=True, padding_side="right")

# pad_token meselesi: checkpoint tokenizer ne diyorsa ona uy
if tokenizer_ckpt.pad_token is None:
    tokenizer_ckpt.pad_token = tokenizer_ckpt.eos_token

torch.cuda.empty_cache()

print("üîÑ Loading base model:", config.MODEL_NAME)
base_model = AutoModelForCausalLM.from_pretrained(
    config.MODEL_NAME,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    attn_implementation="eager",
)

# üîß kritik: vocab'ƒ± checkpoint tokenizer uzunluƒüuna getir
print("üîß Resizing token embeddings to:", len(tokenizer_ckpt))
base_model.resize_token_embeddings(len(tokenizer_ckpt))

print("üîÑ Attaching LoRA adapter from:", CKPT_PATH)
eval_model = PeftModel.from_pretrained(base_model, CKPT_PATH)
eval_model.eval()

# eval trainer
eval_trainer = SFTTrainer(
    model=eval_model,
    args=sft_config,          # sende zaten var
    train_dataset=None,
    eval_dataset=test_dataset, # sende zaten var
    tokenizer=tokenizer_ckpt,
    data_collator=collator,    # sende zaten var
)

print("üìä Evaluating on test set (checkpoint-300)...")
eval_results = eval_trainer.evaluate()

print("\n‚úÖ Test evaluation complete!")
print(f"üìâ Test loss: {eval_results['eval_loss']:.4f}")
print(f"üìà Test perplexity: {torch.exp(torch.tensor(eval_results['eval_loss'])):.2f}")

from google.colab import drive
import shutil
import os


SRC = "./qwen-medical-sign-lora/checkpoint-300"
DST = "/content/drive/MyDrive/qwen-medical-sign-lora/checkpoint-300"

# Eƒüer daha √∂nce varsa sil (√ßakƒ±≈üma olmasƒ±n)
if os.path.exists(DST):
    shutil.rmtree(DST)

shutil.copytree(SRC, DST)

print("‚úÖ checkpoint-300 Drive'a kaydedildi:")
print(DST)

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os, torch

CKPT = f"{config.OUTPUT_DIR}/checkpoint-300"
adapter_path = f"{config.OUTPUT_DIR}/final_adapter_ckpt300"

# tokenizer'ƒ± checkpoint'ten al (vocab aynƒ± olsun)
tokenizer = AutoTokenizer.from_pretrained(CKPT, trust_remote_code=True, padding_side="right")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    config.MODEL_NAME,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    attn_implementation="eager"
)
base_model.resize_token_embeddings(len(tokenizer))

model = PeftModel.from_pretrained(base_model, CKPT)
model.save_pretrained(adapter_path)
tokenizer.save_pretrained(adapter_path)

print("‚úÖ Saved:", adapter_path)
print("Adapter MB:", os.path.getsize(f"{adapter_path}/adapter_model.safetensors")/1e6)

!zip -r qwen_adapter_ckpt300.zip {config.OUTPUT_DIR}/final_adapter_ckpt300

from google.colab import files
files.download("/content/qwen_adapter_ckpt300.zip")

import torch, gc
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

gc.collect()
torch.cuda.empty_cache()

# adapter'ƒ± nereye kaydettiysen onu yaz
adapter_path = "./qwen-medical-sign-lora/final_adapter_ckpt300"  # veya ./final_adapter

tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True, padding_side="right")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",   # config.MODEL_NAME de olur
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    attn_implementation="eager"
)

base_model.resize_token_embeddings(len(tokenizer))

model_with_adapter = PeftModel.from_pretrained(base_model, adapter_path, torch_dtype=torch.bfloat16)

# hƒ±z istersen:
model_with_adapter = model_with_adapter.merge_and_unload()

print("‚úÖ model_with_adapter hazƒ±r")

# ============================================================================
# CELL 15: Inference Function
# ============================================================================

SYSTEM_PROMPT = """You are a medical sign language interpreter AI assistant working in a hospital setting.

Your role is to translate sign language recognition outputs into clear, professional English text that doctors and medical staff can understand.

## Core Principles

1. **Accuracy First**: Only use information explicitly present in the conversation history and current turn
2. **No Fabrication**: Never invent symptoms, diagnoses, time frames, or medical details
3. **Literal Translation Priority**: Default to literal word-by-word translation unless context clearly requires integration
4. **Context Awareness**: Consider conversation history but don't force connections
5. **Professional Tone**: Write in clear, medical-appropriate language

## CRITICAL TRANSLATION RULES

### Rule 1: Literal First Approach
When translating current_turn_selected, start with literal translation

### Rule 2: Context Integration
ONLY integrate previous context when clearly related

### Rule 3: Questions vs Statements
NEVER turn statements into questions unless explicit

### Rule 4: Body Parts
- Body part ALONE = location reference
- Body part + symptom = combined

### Rule 5: Single Word Turns
Use MINIMAL interpretation

## Output Format

Return ONLY valid JSON:
{
  "patient_message": "Natural English sentence",
  "structured_summary": "Bullet-pointed summary"
}"""

def generate_response(
    model,
    tokenizer,
    conversation_history: List[Dict],
    current_turn_selected: List[str],
    max_new_tokens: int = 256,
    temperature: float = 0.1
) -> Dict:
    """Generate response from trained model"""

    user_input = {
        "conversation_history": conversation_history,
        "current_turn_selected": current_turn_selected
    }

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": json.dumps(user_input, ensure_ascii=False)}
    ]

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            top_p=0.95,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

    try:
        return json.loads(response)
    except:
        import re
        match = re.search(r'\{.*\}', response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(0))
            except:
                pass
        return {"patient_message": response, "structured_summary": "Parse error"}

print("‚úÖ Inference function ready!")

# ============================================================================
# CELL 15: Inference Function
# ============================================================================

SYSTEM_PROMPT = """You are a medical sign language interpreter AI assistant working in a hospital setting.

Your role is to translate sign language recognition outputs into clear, professional English text that doctors and medical staff can understand.

## Core Principles

1. **Accuracy First**: Only use information explicitly present in the conversation history and current turn
2. **No Fabrication**: Never invent symptoms, diagnoses, time frames, or medical details
3. **Literal Translation Priority**: Default to literal word-by-word translation unless context clearly requires integration
4. **Context Awareness**: Consider conversation history but don't force connections
5. **Professional Tone**: Write in clear, medical-appropriate language

## CRITICAL TRANSLATION RULES

### Rule 1: Literal First Approach
When translating current_turn_selected, start with literal translation

### Rule 2: Context Integration
ONLY integrate previous context when clearly related

### Rule 3: Questions vs Statements
NEVER turn statements into questions unless explicit

### Rule 4: Body Parts
- Body part ALONE = location reference
- Body part + symptom = combined

### Rule 5: Single Word Turns
Use MINIMAL interpretation

## Output Format

Return ONLY valid JSON:
{
  "patient_message": "Natural English sentence",
  "structured_summary": "Bullet-pointed summary"
}"""

def generate_response(
    model,
    tokenizer,
    conversation_history: List[Dict],
    current_turn_selected: List[str],
    max_new_tokens: int = 256,
    temperature: float = 0.1
) -> Dict:
    """Generate response from trained model"""

    user_input = {
        "conversation_history": conversation_history,
        "current_turn_selected": current_turn_selected
    }

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": json.dumps(user_input, ensure_ascii=False)}
    ]

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            top_p=0.95,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

    try:
        return json.loads(response)
    except:
        import re
        match = re.search(r'\{.*\}', response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(0))
            except:
                pass
        return {"patient_message": response, "structured_summary": "Parse error"}

print("‚úÖ Inference function ready!")

# ============================================================================
# CELL 16: Comprehensive Testing
# ============================================================================

print("üß™ COMPREHENSIVE MODEL TESTING")
print("=" * 80)

test_cases = [
    {
        "name": "Simple symptom",
        "turns": [["i", "pain", "head"]]
    },
    {
        "name": "Single word (literal test)",
        "turns": [["office"]]
    },
    {
        "name": "Multi-turn context",
        "turns": [
            ["i", "sick", "yesterday"],
            ["stomach"],
            ["doctor", "need"]
        ]
    },
    {
        "name": "Family context",
        "turns": [
            ["my", "daughter", "pain"],
            ["fever", "high"],
            ["hospital", "now"]
        ]
    },
    {
        "name": "Body part progression",
        "turns": [
            ["pain"],
            ["head"],
            ["morning"]
        ]
    }
]

for test_case in test_cases:
    print(f"\n\n{'='*80}")
    print(f"TEST: {test_case['name']}")
    print(f"{'='*80}")

    conversation = []

    for turn_num, turn_words in enumerate(test_case['turns'], 1):
        print(f"\n--- Turn {turn_num} ---")
        print(f"Input: {turn_words}")

        result = generate_response(
            model_with_adapter,
            tokenizer,
            conversation_history=conversation,
            current_turn_selected=turn_words
        )

        print(f"\n‚úÖ Response:")
        print(json.dumps(result, indent=2, ensure_ascii=False))

        conversation.append({
            "turn": turn_num,
            "selected": turn_words,
            "state": {"intent": "unknown", "slots": {}}
        })

print("\n\n" + "=" * 80)
print("‚úÖ ALL TESTS COMPLETE!")
print("=" * 80)