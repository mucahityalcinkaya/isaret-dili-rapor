# -*- coding: utf-8 -*-
"""150epoch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dC4_qBE-SZt0o5KQOtDiDLSFUmR7Kjsj
"""

# ============================================================================
# HÃœCRE 1: Drive BaÄŸlantÄ±sÄ± ve Kurulum
# ============================================================================
from google.colab import drive
drive.mount('/content/drive')

!pip install -q einops timm tensorboard scikit-learn

print("âœ“ KÃ¼tÃ¼phaneler yÃ¼klendi")

# ============================================================================
# HÃœCRE 2: Import'lar ve Ayarlar
# ============================================================================
import os
import json
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, top_k_accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import warnings
import zipfile
import shutil
import gc
warnings.filterwarnings('ignore')

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

set_seed(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ðŸŽ® Device: {device}")
print(f"ðŸ’¾ GPU: {torch.cuda.get_device_name(0)}")
print(f"ðŸ“¦ PyTorch: {torch.__version__}")

# ============================================================================
# HÃœCRE 3: Veri Ã‡Ä±kar
# ============================================================================
ZIP_PATH = "/content/drive/MyDrive/unisiggn_output.zip"
EXTRACT_DIR = "/content/dataset"

if not os.path.exists(EXTRACT_DIR):
    print("ðŸ“¦ Ã‡Ä±karÄ±lÄ±yor...")
    os.makedirs(EXTRACT_DIR, exist_ok=True)
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(EXTRACT_DIR)
    print("âœ“ Ã‡Ä±karÄ±ldÄ±")

data_root = EXTRACT_DIR
skeleton_root = os.path.join(data_root, "skeleton")
class_to_idx_file = os.path.join(data_root, "class_to_idx.json")

with open(class_to_idx_file, 'r') as f:
    class_to_idx = json.load(f)

idx_to_class = {v: k for k, v in class_to_idx.items()}
num_classes = len(class_to_idx)
print(f"âœ“ {num_classes} sÄ±nÄ±f yÃ¼klendi")

# ============================================================================
# HÃœCRE 4: Dengeli Train/Val Split (HER SINIF MIN 2 VAL)
# ============================================================================
print("\n" + "="*70)
print("ðŸ“Š DENGELI TRAIN/VAL SPLIT OLUÅžTURULUYOR")
print("="*70)

# TÃ¼m videolarÄ± topla
all_samples = defaultdict(list)
for cls_name in class_to_idx.keys():
    cls_dir = os.path.join(skeleton_root, cls_name)
    if not os.path.exists(cls_dir):
        continue

    videos = [f for f in os.listdir(cls_dir) if f.endswith('.npy')]
    for video in videos:
        video_stem = video.replace('.npy', '')
        all_samples[cls_name].append(video_stem)

# Stratejik split: Her sÄ±nÄ±f iÃ§in min 2 val
MIN_VAL_PER_CLASS = 2
VAL_RATIO = 0.20

train_samples = []
val_samples = []
split_stats = []

random.seed(42)

for cls_name, videos in all_samples.items():
    n_total = len(videos)

    # Rastgele karÄ±ÅŸtÄ±r
    videos_shuffled = videos.copy()
    random.shuffle(videos_shuffled)

    # Val sayÄ±sÄ±nÄ± belirle
    if n_total < MIN_VAL_PER_CLASS:
        # Ã‡ok az video var, val'e koyma
        n_val = 0
        n_train = n_total
    elif n_total < 5:
        # 2-4 video var, 1 val
        n_val = 1
        n_train = n_total - 1
    else:
        # 5+ video var, en az 2 val
        n_val = max(MIN_VAL_PER_CLASS, int(n_total * VAL_RATIO))
        n_train = n_total - n_val

    # Split yap
    val_vids = videos_shuffled[:n_val]
    train_vids = videos_shuffled[n_val:]

    # Listeye ekle
    for vid in train_vids:
        train_samples.append(f"{cls_name}/{vid}")
    for vid in val_vids:
        val_samples.append(f"{cls_name}/{vid}")

    # Ä°statistik
    split_stats.append({
        'Class': cls_name,
        'Total': n_total,
        'Train': n_train,
        'Val': n_val,
        'Val_Ratio': n_val/n_total if n_total > 0 else 0
    })

# DataFrame oluÅŸtur
df_split = pd.DataFrame(split_stats)

print(f"\nðŸ“Š Split Ä°statistikleri:")
print(f"âœ“ Toplam SÄ±nÄ±f: {len(all_samples)}")
print(f"âœ“ Train Samples: {len(train_samples)}")
print(f"âœ“ Val Samples: {len(val_samples)}")
print(f"âœ“ Val Ratio: {len(val_samples)/(len(train_samples)+len(val_samples)):.2%}")

print(f"\nðŸ“Š Val Sample DaÄŸÄ±lÄ±mÄ±:")
val_dist = df_split['Val'].value_counts().sort_index()
for val_count, class_count in val_dist.items():
    print(f"  {val_count} val sample: {class_count} sÄ±nÄ±f")

print(f"\nâš ï¸ Val'de 0 sample olan sÄ±nÄ±flar:")
no_val = df_split[df_split['Val'] == 0]
print(f"  Toplam: {len(no_val)} sÄ±nÄ±f")
if len(no_val) > 0:
    print(f"  Ã–rnekler: {', '.join(no_val['Class'].head(10).tolist())}")

print(f"\nâœ… Val'de 2+ sample olan sÄ±nÄ±flar: {len(df_split[df_split['Val'] >= 2])} / {len(df_split)}")

# GÃ¶rselleÅŸtir
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# 1. Total daÄŸÄ±lÄ±m
axes[0].hist(df_split['Total'], bins=30, color='skyblue', edgecolor='black')
axes[0].axvline(df_split['Total'].median(), color='red', linestyle='--',
                label=f"Median: {df_split['Total'].median():.0f}")
axes[0].set_xlabel('Video SayÄ±sÄ±')
axes[0].set_ylabel('SÄ±nÄ±f SayÄ±sÄ±')
axes[0].set_title('SÄ±nÄ±f BaÅŸÄ±na Toplam Video')
axes[0].legend()
axes[0].grid(alpha=0.3)

# 2. Train daÄŸÄ±lÄ±m
axes[1].hist(df_split['Train'], bins=30, color='lightgreen', edgecolor='black')
axes[1].axvline(df_split['Train'].median(), color='red', linestyle='--',
                label=f"Median: {df_split['Train'].median():.0f}")
axes[1].set_xlabel('Video SayÄ±sÄ±')
axes[1].set_ylabel('SÄ±nÄ±f SayÄ±sÄ±')
axes[1].set_title('SÄ±nÄ±f BaÅŸÄ±na Train Video')
axes[1].legend()
axes[1].grid(alpha=0.3)

# 3. Val daÄŸÄ±lÄ±m
axes[2].hist(df_split['Val'], bins=20, color='coral', edgecolor='black')
axes[2].axvline(df_split['Val'].median(), color='red', linestyle='--',
                label=f"Median: {df_split['Val'].median():.0f}")
axes[2].set_xlabel('Video SayÄ±sÄ±')
axes[2].set_ylabel('SÄ±nÄ±f SayÄ±sÄ±')
axes[2].set_title('SÄ±nÄ±f BaÅŸÄ±na Val Video')
axes[2].legend()
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('/content/split_distribution.png', dpi=150, bbox_inches='tight')
plt.show()

# CSV kaydet
df_split.to_csv('/content/split_statistics.csv', index=False)
print("\nâœ“ Split istatistikleri kaydedildi")

# Yeni split dosyalarÄ±nÄ± kaydet
splits_dir = os.path.join(data_root, "splits_balanced")
os.makedirs(splits_dir, exist_ok=True)

with open(os.path.join(splits_dir, "train.txt"), "w") as f:
    for sample in train_samples:
        f.write(f"{sample}\n")

with open(os.path.join(splits_dir, "val.txt"), "w") as f:
    for sample in val_samples:
        f.write(f"{sample}\n")

print(f"âœ“ Yeni split dosyalarÄ± kaydedildi: {splits_dir}")

# ============================================================================
# HÃœCRE 5: Moderate Augmentation Dataset
# ============================================================================
class WLASLModerateDataset(Dataset):
    """Orta dÃ¼zeyde augmentation'lÄ± dataset"""
    def __init__(self, data_root, samples_list, class_to_idx, augment=False):
        self.skeleton_root = os.path.join(data_root, "skeleton")
        self.samples = samples_list
        self.class_to_idx = class_to_idx
        self.augment = augment

        print(f"âœ“ {len(self.samples)} sample yÃ¼klendi (augment={augment})")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        cls_name, video_name = sample.split('/')

        npy_path = os.path.join(self.skeleton_root, cls_name, video_name + '.npy')
        data = np.load(npy_path).astype(np.float32)  # (T=64, V=51, C=3)

        if self.augment:
            data = self.moderate_augment(data)

        label = self.class_to_idx[cls_name]
        data = torch.from_numpy(data).permute(2, 0, 1)  # (C, T, V)

        return data, label

    def moderate_augment(self, data):
        """Orta dÃ¼zeyde augmentation (daha az agresif)"""
        T, V, C = data.shape

        # 1. Spatial Flip (en Ã¶nemli)
        if random.random() < 0.5:
            lh = data[:, :21, :].copy()
            rh = data[:, 21:42, :].copy()
            data[:, :21, :] = rh
            data[:, 21:42, :] = lh
            data[:, :, 0] = -data[:, :, 0]

        # 2. Random Scale (hafif)
        if random.random() < 0.3:
            scale = random.uniform(0.90, 1.10)
            data[:, :, :2] *= scale

        # 3. Temporal Jitter (hafif)
        if random.random() < 0.25:
            shift = random.randint(-4, 4)
            data = np.roll(data, shift, axis=0)

        # 4. Small Gaussian Noise (Ã§ok hafif)
        if random.random() < 0.2:
            noise = np.random.normal(0, 0.005, data[:, :, :2].shape).astype(np.float32)
            data[:, :, :2] += noise

        # 5. Random Rotation (Ã§ok kÃ¼Ã§Ã¼k)
        if random.random() < 0.2:
            angle = random.uniform(-10, 10) * np.pi / 180
            cos_a, sin_a = np.cos(angle), np.sin(angle)
            rot_matrix = np.array([[cos_a, -sin_a], [sin_a, cos_a]])
            for t in range(T):
                data[t, :, :2] = data[t, :, :2] @ rot_matrix.T

        return data

print("âœ“ Moderate Augmentation Dataset hazÄ±r")

# ============================================================================
# HÃœCRE 6: Model (Ã–nceki BaÅŸarÄ±lÄ± Model)
# ============================================================================
class ImprovedSignModel(nn.Module):
    """Ã–nceki baÅŸarÄ±lÄ± model (hidden=128, layer=2)"""
    def __init__(self, num_classes=249, num_frames=64, num_joints=51):
        super().__init__()

        # Spatial feature extractor
        self.spatial_conv = nn.Sequential(
            nn.Conv1d(3, 64, kernel_size=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Conv1d(64, 128, kernel_size=1),
            nn.BatchNorm1d(128),
            nn.ReLU()
        )

        # Temporal feature extractor
        self.temporal_conv = nn.Sequential(
            nn.Conv1d(128 * num_joints, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Conv1d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

        self.num_joints = num_joints

    def forward(self, x):
        # x: (B, C=3, T=64, V=51)
        B, C, T, V = x.shape

        # Spatial: process each frame
        x = x.permute(0, 2, 1, 3).reshape(B * T, C, V)
        x = self.spatial_conv(x)

        # Reshape back
        x = x.reshape(B, T, -1).permute(0, 2, 1)

        # Temporal
        x = self.temporal_conv(x)
        x = x.squeeze(-1)

        # Classify
        x = self.classifier(x)
        return x

model = ImprovedSignModel(num_classes=num_classes).to(device)
total_params = sum(p.numel() for p in model.parameters())
print(f"ðŸ“Š Model Parametreleri: {total_params:,}")

dummy = torch.randn(2, 3, 64, 51).to(device)
out = model(dummy)
print(f"âœ“ Model Test: {dummy.shape} -> {out.shape}")

# ============================================================================
# HÃœCRE 7: DataLoader
# ============================================================================
train_dataset = WLASLModerateDataset(data_root, train_samples, class_to_idx, augment=True)
val_dataset = WLASLModerateDataset(data_root, val_samples, class_to_idx, augment=False)

print(f"âœ“ Train: {len(train_dataset)}, Val: {len(val_dataset)}")

# Class weights
train_labels = [class_to_idx[s.split('/')[0]] for s in train_samples]
class_counts = Counter(train_labels)
class_weights_tensor = torch.ones(num_classes)
for cls, count in class_counts.items():
    class_weights_tensor[cls] = 1.0 / count
class_weights_tensor = class_weights_tensor / class_weights_tensor.sum() * num_classes
class_weights_tensor = class_weights_tensor.to(device)

print(f"âœ“ Class weights: min={class_weights_tensor.min():.3f}, max={class_weights_tensor.max():.3f}")

# DataLoader
BATCH_SIZE = 8  # A100 iÃ§in artÄ±rÄ±ldÄ±
NUM_WORKERS = 4

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                        num_workers=NUM_WORKERS, pin_memory=True)

print(f"âœ“ Batch Size: {BATCH_SIZE}")
print(f"âœ“ Train Batches: {len(train_loader)}, Val Batches: {len(val_loader)}")

# ============================================================================
# HÃœCRE 8: Training Setup
# ============================================================================
EPOCHS = 150
WARMUP_EPOCHS = 10
MIN_DELTA = 0.001
PATIENCE = 15

# Loss
criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.05)

# Optimizer (LR artÄ±rÄ±ldÄ±)
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1.5e-3,  # 1e-3'ten 1.5e-3'e artÄ±rÄ±ldÄ±
    weight_decay=1e-4,
    betas=(0.9, 0.999)
)

# Warmup + Cosine Scheduler
def get_lr(epoch, warmup_epochs, max_epochs, base_lr, min_lr=1e-6):
    if epoch < warmup_epochs:
        return base_lr * (epoch + 1) / warmup_epochs
    else:
        progress = (epoch - warmup_epochs) / (max_epochs - warmup_epochs)
        return min_lr + (base_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))

writer = SummaryWriter('/content/runs/wlasl_balanced')

print("âœ“ Training setup hazÄ±r")
print(f"ðŸ“Š Epochs: {EPOCHS}, Warmup: {WARMUP_EPOCHS}")
print(f"ðŸ“Š Base LR: 1.5e-3 (artÄ±rÄ±ldÄ±)")
print(f"ðŸ“Š Early Stopping: Patience={PATIENCE}, Min Delta={MIN_DELTA}")

# ============================================================================
# HÃœCRE 9: Training Loop
# ============================================================================
def train_epoch(model, loader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    all_preds, all_labels = [], []

    pbar = tqdm(loader, desc=f'Epoch {epoch+1:3d} [Train]')
    for data, labels in pbar:
        data, labels = data.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        running_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

        pbar.set_postfix({'loss': f'{loss.item():.4f}'})

    return running_loss / len(loader), accuracy_score(all_labels, all_preds)

@torch.no_grad()
def validate_epoch(model, loader, criterion, device, epoch):
    model.eval()
    running_loss = 0.0
    all_preds, all_labels, all_probs = [], [], []

    pbar = tqdm(loader, desc=f'Epoch {epoch+1:3d} [Val]  ')
    for data, labels in pbar:
        data, labels = data.to(device), labels.to(device)

        outputs = model(data)
        loss = criterion(outputs, labels)

        running_loss += loss.item()
        probs = F.softmax(outputs, dim=1)
        _, preds = torch.max(outputs, 1)

        all_probs.extend(probs.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

        pbar.set_postfix({'loss': f'{loss.item():.4f}'})

    all_probs = np.array(all_probs)
    all_labels = np.array(all_labels)

    epoch_loss = running_loss / len(loader)
    epoch_acc = accuracy_score(all_labels, all_preds)
    top5_acc = top_k_accuracy_score(all_labels, all_probs, k=5, labels=range(num_classes))

    return epoch_loss, epoch_acc, top5_acc, all_preds, all_labels

# Training
best_val_acc = 0.0
patience_counter = 0
history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_top5_acc': [], 'lr': []}

print("\nðŸš€ EÄŸitim baÅŸlÄ±yor...")
print("="*70)

for epoch in range(EPOCHS):
    # Update LR
    current_lr = get_lr(epoch, WARMUP_EPOCHS, EPOCHS, base_lr=1.5e-3, min_lr=1e-6)
    for param_group in optimizer.param_groups:
        param_group['lr'] = current_lr

    # Train & Validate
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)
    val_loss, val_acc, val_top5_acc, val_preds, val_labels = validate_epoch(
        model, val_loader, criterion, device, epoch
    )

    # History
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    history['val_top5_acc'].append(val_top5_acc)
    history['lr'].append(current_lr)

    # TensorBoard
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)
    writer.add_scalar('Accuracy/train', train_acc, epoch)
    writer.add_scalar('Accuracy/val', val_acc, epoch)
    writer.add_scalar('Accuracy/val_top5', val_top5_acc, epoch)
    writer.add_scalar('LR', current_lr, epoch)

    # Print
    print(f"\nEpoch {epoch+1}/{EPOCHS}")
    print(f"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}")
    print(f"  Val:   Loss={val_loss:.4f}, Top-1={val_acc:.4f}, Top-5={val_top5_acc:.4f}")
    print(f"  LR: {current_lr:.6f}")

    # Save best model
    if val_acc > best_val_acc + MIN_DELTA:
        improvement = val_acc - best_val_acc
        best_val_acc = val_acc
        patience_counter = 0
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
            'val_top5_acc': val_top5_acc,
        }, '/content/best_model.pth')
        print(f"  âœ… Best model saved! (Î”={improvement:.4f})")
    else:
        patience_counter += 1
        print(f"  â³ Patience: {patience_counter}/{PATIENCE}")

    # Early stopping
    if patience_counter >= PATIENCE:
        print(f"\nâš ï¸ Early stopping! No improvement for {PATIENCE} epochs.")
        break

    # Memory cleanup
    if epoch % 10 == 0:
        torch.cuda.empty_cache()
        gc.collect()

print("\nâœ… EÄŸitim tamamlandÄ±!")
print(f"ðŸ† Best Val Accuracy (Top-1): {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")

writer.close()

# ============================================================================
# HÃœCRE 10: GÃ¶rselleÅŸtirme
# ============================================================================
plt.figure(figsize=(18, 5))

plt.subplot(1, 4, 1)
plt.plot(history['train_loss'], label='Train', linewidth=2)
plt.plot(history['val_loss'], label='Val', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss')
plt.legend()
plt.grid(alpha=0.3)

plt.subplot(1, 4, 2)
plt.plot(history['train_acc'], label='Train', linewidth=2)
plt.plot(history['val_acc'], label='Val', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Top-1 Accuracy')
plt.legend()
plt.grid(alpha=0.3)

plt.subplot(1, 4, 3)
plt.plot(history['val_top5_acc'], color='green', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Val Top-5 Accuracy')
plt.grid(alpha=0.3)

plt.subplot(1, 4, 4)
plt.plot(history['lr'], color='orange', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('LR Schedule (Warmup+Cosine)')
plt.yscale('log')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('/content/training_history_balanced.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… Grafikler kaydedildi")

# ============================================================================
# HÃœCRE 11: Final Evaluation with Top-K Analysis
# ============================================================================
checkpoint = torch.load('/content/best_model.pth', weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
print(f"âœ… Best model yÃ¼klendi (Epoch {checkpoint['epoch']+1})")

# DetaylÄ± validation
@torch.no_grad()
def detailed_validate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    all_preds, all_labels, all_probs = [], [], []

    pbar = tqdm(loader, desc='Final Validation')
    for data, labels in pbar:
        data, labels = data.to(device), labels.to(device)

        outputs = model(data)
        loss = criterion(outputs, labels)

        running_loss += loss.item()
        probs = F.softmax(outputs, dim=1)
        _, preds = torch.max(outputs, 1)

        all_probs.extend(probs.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    all_probs = np.array(all_probs)
    all_labels = np.array(all_labels)

    epoch_loss = running_loss / len(loader)

    # Top-K accuracies (1'den 10'a kadar)
    top_k_accs = {}
    for k in range(1, 11):
        top_k_acc = top_k_accuracy_score(all_labels, all_probs, k=k, labels=range(num_classes))
        top_k_accs[k] = top_k_acc

    return epoch_loss, top_k_accs, all_preds, all_labels, all_probs

val_loss, top_k_accs, val_preds, val_labels, val_probs = detailed_validate(
    model, val_loader, criterion, device
)

print("\n" + "="*70)
print("ðŸ“Š FINAL RESULTS - TOP-K ACCURACY")
print("="*70)
print(f"âœ… Validation Loss: {val_loss:.4f}")
print("\nðŸ“ˆ Top-K Accuracy DetaylarÄ±:")
print("-"*70)
for k in range(1, 11):
    acc = top_k_accs[k]
    bar_length = int(acc * 50)
    bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
    print(f"  Top-{k:2d}: {acc:.4f} ({acc*100:5.2f}%) {bar}")
print("="*70)

# Top-K grafiÄŸi
plt.figure(figsize=(12, 6))

# Bar plot
plt.subplot(1, 2, 1)
ks = list(range(1, 11))
accs = [top_k_accs[k] * 100 for k in ks]
bars = plt.bar(ks, accs, color='steelblue', edgecolor='black', alpha=0.8)

# Renk gradyanÄ±
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, 10))
for bar, color in zip(bars, colors):
    bar.set_color(color)

plt.xlabel('K', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.title('Top-K Accuracy (K=1 to 10)', fontsize=14, fontweight='bold')
plt.xticks(ks)
plt.ylim(0, 100)
plt.grid(axis='y', alpha=0.3)

# DeÄŸerleri bar Ã¼stÃ¼ne yaz
for i, (k, acc) in enumerate(zip(ks, accs)):
    plt.text(k, acc + 1.5, f'{acc:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')

# Line plot
plt.subplot(1, 2, 2)
plt.plot(ks, accs, marker='o', linewidth=2.5, markersize=8, color='steelblue')
plt.fill_between(ks, accs, alpha=0.3, color='steelblue')
plt.xlabel('K', fontsize=12)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.title('Top-K Accuracy Progression', fontsize=14, fontweight='bold')
plt.xticks(ks)
plt.ylim(0, 100)
plt.grid(alpha=0.3)

# Ä°yileÅŸme oranlarÄ±nÄ± ekle
for i in range(1, len(ks)):
    improvement = accs[i] - accs[i-1]
    mid_x = (ks[i-1] + ks[i]) / 2
    mid_y = (accs[i-1] + accs[i]) / 2
    plt.text(mid_x, mid_y, f'+{improvement:.1f}%', fontsize=8,
             ha='center', color='red', fontweight='bold')

plt.tight_layout()
plt.savefig('/content/topk_accuracy.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nâœ… Top-K grafikleri kaydedildi: /content/topk_accuracy.png")

# Per-class analysis
class_correct = Counter()
class_total = Counter()
for pred, label in zip(val_preds, val_labels):
    class_total[label] += 1
    if pred == label:
        class_correct[label] += 1

# Val sample sayÄ±sÄ±nÄ± da ekle
val_sample_counts = Counter([class_to_idx[s.split('/')[0]] for s in val_samples])

class_results = []
for cls_id in class_total.keys():
    cls_name = idx_to_class[cls_id]
    acc = class_correct[cls_id] / class_total[cls_id]
    n_val = val_sample_counts[cls_id]
    class_results.append((cls_name, acc, n_val, class_total[cls_id]))

class_results.sort(key=lambda x: x[1], reverse=True)

print("\nðŸ† En Ä°yi 20 SÄ±nÄ±f:")
for i, (cls, acc, n_val, n_tested) in enumerate(class_results[:20], 1):
    print(f"  {i:2d}. {cls:20s}: {acc:.3f} ({acc*100:5.1f}%) | Val samples={n_val}")

print("\nâš ï¸ En Zor 20 SÄ±nÄ±f:")
for i, (cls, acc, n_val, n_tested) in enumerate(class_results[-20:], 1):
    print(f"  {i:2d}. {cls:20s}: {acc:.3f} ({acc*100:5.1f}%) | Val samples={n_val}")

# Top-K breakdown by confidence
print("\n" + "="*70)
print("ðŸŽ¯ TOP-K GÃœVEN ANALÄ°ZÄ°")
print("="*70)

# Her sample iÃ§in top-5 tahminleri analiz et
confidence_analysis = {k: [] for k in range(1, 6)}
for i, (probs, true_label) in enumerate(zip(val_probs, val_labels)):
    # Top-5 tahmin
    top5_indices = np.argsort(probs)[-5:][::-1]
    top5_probs = probs[top5_indices]

    # DoÄŸru tahmin kaÃ§Ä±ncÄ± sÄ±rada?
    if true_label in top5_indices:
        rank = np.where(top5_indices == true_label)[0][0] + 1
        confidence_analysis[rank].append(top5_probs[rank-1])

print("\nDoÄŸru tahminin sÄ±ralamasÄ± ve ortalama gÃ¼ven:")
for k in range(1, 6):
    if len(confidence_analysis[k]) > 0:
        avg_conf = np.mean(confidence_analysis[k])
        count = len(confidence_analysis[k])
        percentage = count / len(val_labels) * 100
        print(f"  Top-{k}: {count:4d} Ã¶rnek ({percentage:5.2f}%) | Ort. GÃ¼ven: {avg_conf:.4f} ({avg_conf*100:.2f}%)")
    else:
        print(f"  Top-{k}: {0:4d} Ã¶rnek ({0:5.2f}%)")

print("="*70)

# ============================================================================
# HÃœCRE 12: Drive'a Kaydet
# ============================================================================
save_dir = "/content/drive/MyDrive/wlasl_balanced_results"
os.makedirs(save_dir, exist_ok=True)

shutil.copy('/content/best_model.pth', os.path.join(save_dir, 'best_model_balanced.pth'))
shutil.copy('/content/training_history_balanced.png', os.path.join(save_dir, 'training_history_balanced.png'))
shutil.copy('/content/split_distribution.png', os.path.join(save_dir, 'split_distribution.png'))
shutil.copy('/content/split_statistics.csv', os.path.join(save_dir, 'split_statistics.csv'))

history_df = pd.DataFrame(history)
shutil.copy('/content/topk_accuracy.png', os.path.join(save_dir, 'topk_accuracy.png'))

# Top-K sonuÃ§larÄ±nÄ± da kaydet
topk_results = {f'top_{k}': float(top_k_accs[k]) for k in range(1, 11)}

history_df.to_csv(os.path.join(save_dir, 'training_history_balanced.csv'), index=False)

final_results = {
    'val_loss': float(val_loss),
    'best_epoch': int(checkpoint['epoch']),
    'num_classes': num_classes,
    'num_train_samples': len(train_samples),
    'num_val_samples': len(val_samples),
    'model_params': total_params,
    'batch_size': BATCH_SIZE,
    'base_lr': 1.5e-3,
    'val_min_per_class': MIN_VAL_PER_CLASS,
    **topk_results  # Top-1 to Top-10 ekle
}

with open(os.path.join(save_dir, 'final_results_balanced.json'), 'w') as f:
    json.dump(final_results, f, indent=2)

print(f"\nâœ… TÃ¼m sonuÃ§lar kaydedildi: {save_dir}")
print("\nðŸ“¦ Kaydedilen dosyalar:")
print("  - best_model_balanced.pth")
print("  - training_history_balanced.png")
print("  - training_history_balanced.csv")
print("  - split_distribution.png")
print("  - split_statistics.csv")
print("  - topk_accuracy.png")
print("  - final_results_balanced.json")

# Top-K Ã¶zet
print("\n" + "="*70)
print("ðŸ“Š TOP-K ACCURACY Ã–ZET")
print("="*70)
for k in range(1, 11):
    print(f"  Top-{k:2d}: {top_k_accs[k]*100:5.2f}%")
print("="*70)